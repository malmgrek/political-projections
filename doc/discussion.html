<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<title>discussion.html</title>
<meta http-equiv="Content-Type" content="application/xhtml+xml;charset=utf-8"/>
<link rel="stylesheet" type="text/css" media="all" href="https://cdn.jsdelivr.net/npm/github-markdown-css/github-markdown.min.css"  />
<link rel="stylesheet" type="text/css" media="all" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release/build/styles/github.min.css"  /><meta name='viewport' content='width=device-width, initial-scale=1, shrink-to-fit=no'><style> body { box-sizing: border-box; max-width: 740px; width: 100%; margin: 40px auto; padding: 0 10px; } </style><script id='MathJax-script' async src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'></script><script src='https://cdn.jsdelivr.net/gh/highlightjs/cdn-release/build/highlight.min.js'></script><script>document.addEventListener('DOMContentLoaded', () => { document.body.classList.add('markdown-body'); document.querySelectorAll('pre[lang] > code').forEach((code) => { code.classList.add(code.parentElement.lang); }); document.querySelectorAll('pre > code').forEach((code) => { hljs.highlightBlock(code); }); });</script>
</head>

<body>

<h1 id="chapel-hill-2019-discussion-of-results">Chapel Hill 2019 – discussion of results</h1>
<p>The goal of the project was to implement a 2D dimensionality reduction for the political parties scores in the Chapell Hill 2019 dataset. Furthermore, a model that can be used to “random generate” hypothetical political parties based on their 2D distribution (w.r.t. the dimension reduction method). Principal component analysis (PCA) together with data scaling and feature clustering/selection was studied the most but some experiments with Independent component analysis (ICA) and Factorial analysis (FA) were also done, and the methods are included in the Dash dashboard. Below we only discuss the PCA related findings.</p>
<h2 id="key-findings">Key findings</h2>
<ul>
<li>First component: Anti-immigration + nationalist + Authoritarian + conservative &lt;-&gt; Liberal</li>
<li>Second component: Traditional Left / Right</li>
<li>Extreme right seems to form a separate cluster in 2D</li>
</ul>
<h2 id="analysis-pipeline">Analysis pipeline</h2>
<h3 id="data-preparation">1. Data preparation</h3>
<h4 id="manual-feature-overview">1.1 Manual feature overview</h4>
<p>Most of the questions are directly related to the characterizing the parties but some were</p>
<ul>
<li>metadata such as party nomenclature and respondent date of birth,</li>
<li>so called “vignette” questions which can be used to benchmark how biased the respondents are.</li>
</ul>
<p>Such features were dropped out from this analysis. Obviously the approach doesn’t scale well but it’s important to try to understand the data if possible.</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Possible improvements:</strong> Use the vignette questions to remove respondent biases from the data. Could perhaps be done using e.g. linear regression.</td>
</tr>
</tbody>
</table>
<h4 id="clean-up-data">1.2 Clean up data</h4>
<ol type="1">
<li>Filter NaN’s and transform to numeric data type.</li>
<li>Group by party identifiers with median (or mean).</li>
</ol>
<p>Point 1 is just a routine action but 2 is very important. The point is we primarily want to study the parties and not the respondents views. If we didn’t group the data as above, we would be solving the wrong problem. Moreover, some parties with most evaluations are from quite small countries such as Croatia or Czech republic so the raw data based analysis would be biased towards them.</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Possible improvements:</strong> Not all parties’ observations are equally “noisy”. Some are much more frequent than others, and group variances vary. Hence a noise adaptive method such as weighted PCA would make sense. Left for future studies.</td>
</tr>
</tbody>
</table>
<h4 id="scale-features-and-optionally-standardize">1.3 Scale features and optionally standardize</h4>
<p>The Chapel Hill documents tell the question specific scales – the scales are 1–10, 0–10, 1–7, …, that is, the data need to be scaled. Two unit transformations were experimented:</p>
<ol type="A">
<li>Affine transformation to the interval [-1, 1]</li>
<li>Affine transformation to zero mean and unit variance (standardization)</li>
</ol>
<p>Method A is useful if we want to preserve the information on varying spreads among features. For example, immigration views divide perhaps more than views on religious freedom. Method A is useful if we want to study correlation, not covariance.</p>
<h4 id="re-order-features-based-on-hierarchicaly-clustering">Re-order features based on hierarchicaly clustering</h4>
<p>It is not easy to interpret the principal components information content if the features are in random order. Thus I ran a hierarchical clustering + dendrograph that groups together features whose correlations with other features are similar as vectors. With this done, it is interesting to look at the correlation/covariance matrix as image and compare it with the principal components. It also enables grouping features if desired.</p>
<h3 id="dimensionality-reduction">2. Dimensionality reduction</h3>
<p>Scikit-learn provides an out of the box PCA object although calculating a rudimentary (non-probabilistic) PCA is very straightforward: it is characterized by the eigendecomposition of the covariance matrix. Dimensionality reduction can be calculated by (centering +) projecting the original N-dimensional vectors to a couple of first eigenvectors.</p>
<h4 id="questions-hard-limits-in-2d">2.1 Questions’ hard limits in 2D</h4>
<p>The hard limits are actually orthogonal planes in the N-dimensional space so the survey region is a cuboid. It is enough to find the intersection of each plane with the “<code>xy</code>-plane” defined by the first two principal components. This can be solved analytically (see <code>analysis.py</code>) by expressing a plane in form <code>normal . x = a . x</code>. Since the PCA transformation is <code>Translation + Rotation</code>, its <code>normal</code> is just rotated whereas <code>a</code> is projected onto the line spanned by <code>normal</code>, and rotated.</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Remark:</strong> Some of the <em>projected</em> data points may be out of the <em>projected</em> bounds in 2D because the remaining dimensions are ignored.</td>
</tr>
</tbody>
</table>
<h3 id="kernel-density-estimation-in-2d">3. Kernel density estimation in 2D</h3>
<p>To random draw “new political parties”, I fitted a kernel density function over the 2 projected point cloud using grid search to fine tune smoothness parameter. Scikit-learn’s KDE tool is good as it provides also an API for random sampling from the fitted distribution. The fit is a linear combination of bell surfaces, so it can produce random samples outside of the original hard limits.</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Possible improvements:</strong> Mathematically rigorous way of restricting the probability distribution.</td>
</tr>
</tbody>
</table>
<h3 id="transform-back-to-original-dimensions">4. Transform back to original dimensions</h3>
<p>Points in the reduced dimension can be transformed to the original dimensions with pseudoinverse that maps to the minimum norm solution. Unit transformation is a one-to-one mapping. To ensure “physical” outcomes in original coordinates, we clip to min/max limits and round to nearest integer. This is not exactly accurate but will do for now.</p>
<h2 id="applications">Applications</h2>
<p>For demonstration purposes, I wanted to implement a semi-realistic web application that could be used for visualizing results. In fact, considering the survey data size and unchanging nature, the application is light weight on back-end. Due to the light-weight nature of the methodology, back-end processing resource usage can be reduced to low level through memoization.</p>
<p>If the service would have million or so users per hour, the main bottleneck would be the data traffic load on back-end server as well as network. I would consider using e.g. the Kubernetes Engine to serve a containerized application in a dynamically scaling cluster and Ingress for load balancing the network traffic.</p>
<!-- Millions users: load balancing, -->
<!-- multiple machines, Kubernetes -->
<!-- - Load balancing: -->
<!--   reverse proxy distributes the requests -->
<!-- - defines resources before hand -->
<!-- - master node -> worker nodes -->
<!-- - in principle, scales w.r.t. load -->
<!-- - does the master proxy know loads of subjects? -->
<!-- - Google Kubernetes Engine out of the box solution -->
<!-- ## Dimensionality reduction of Chapel Hill 2019 dataset -->
<!-- The Chapel Hill dataset -->
<!-- [CHESDATA](https://www.chesdata.eu/2019-chapel-hill-expert-survey) contains -->
<!-- survey data regarding different political parties in EU member states. The -->
<!-- respondents are political science experts, and the questions cover a wide range -->
<!-- of topics related to views about economy, social policy, culture, immigration -->
<!-- and so on. -->
<!-- ### Remarks on the dataset -->
<!-- - The survey has been completed by political scientists, and not directly by party -->
<!-- members. -->
<!-- - Some columns are unrelated to -->
<!-- ### Methodology -->
<!-- - Data always centered -->
<!-- - Standardization not a good idea if the aim is to be able to produce an -->
<!--   interesting set of "random parties". Otherwise the difference in variances -->
<!--   will be hidden although it is interesting. -->
<!-- ### Notes -->
<!-- - Because of curse of dimensionality, it is hard to model density in higher dimensions -->
<!-- - It is hard to sample in higher dimensions -->
<!-- - In far-right--liberal axis most variation in absolute terms -->
<!-- - In traditional left-right axis most correlation -->
<!-- - Span of smaller components can be added to samples and they will still inverse transform -->
<!--   to same values -->
<!-- - When projected, some original data points fall outside of the polygon. This as expected, -->
<!--   as the projection ignores the rest of the PCA coordinates. In particular, if the other coordinates -->
<!--   are used, the points will of course inverse map back to the bounds. If we set other components to 0, -->
<!--   then the projected points will map outside of the bounds! -->
<!-- - Separate md file with answers to questions -->
<!-- - Run full pipeline: Clone repo -> make podman-up -> use app -->

</body>
</html>
